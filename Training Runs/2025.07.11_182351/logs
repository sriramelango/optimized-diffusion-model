2025-07-11 18:23:51,992 - Training run started at: 2025.07.11_182351
2025-07-11 18:23:51,992 - Run directory: Training Runs/2025.07.11_182351
2025-07-11 18:23:52,025 - NCSNpp(
  (act): SiLU()
  (time_embed): GaussianFourierProjection()
  (time_mlp): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): SiLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (label_emb): Linear(in_features=1, out_features=256, bias=True)
  (input_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (down_blocks): ModuleList(
    (0-1): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
    (2): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (3-5): 3 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
  )
  (down_attn): ModuleList(
    (0-1): 2 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
    (2-5): 4 x None
  )
  (downsample): ModuleList(
    (0): Downsample(
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))
    )
    (1): Downsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    )
    (2): None
  )
  (mid_block1): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (mid_block2): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (up_blocks): ModuleList(
    (0-5): 6 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)
      (Conv_0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (6): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 192, eps=1e-06, affine=True)
      (Conv_0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (7-8): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
  )
  (up_attn): ModuleList(
    (0-5): 6 x None
    (6-8): 3 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
  )
  (upsample): ModuleList(
    (0-1): 2 x Upsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): None
  )
  (out_norm): GroupNorm(16, 64, eps=1e-06, affine=True)
  (out_act): SiLU()
  (out_conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2025-07-11 18:23:52,026 - EMA: <models.ema.ExponentialMovingAverage object at 0x31538d850>
2025-07-11 18:23:52,026 - Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0004
    maximize: False
    weight_decay: 0
)
2025-07-11 18:23:52,026 - Scaler: None.
2025-07-11 18:23:52,026 - No checkpoint found at Training Runs/2025.07.11_182351/checkpoints-meta/checkpoint.pth. Returned the same state as input
2025-07-11 18:23:52,043 - Starting training loop at step 0.
2025-07-11 18:23:59,394 - step: 0, training_loss: 2.16956e+01
2025-07-11 18:24:00,685 - step: 0, evaluation_loss: 2.03121e+01
2025-07-11 18:24:07,564 - step: 1, training_loss: 2.05239e+01
2025-07-11 18:24:14,432 - step: 2, training_loss: 2.06280e+01
2025-07-11 18:24:21,355 - step: 3, training_loss: 2.05075e+01
2025-07-11 18:24:28,214 - step: 4, training_loss: 2.09347e+01
2025-07-11 18:24:35,171 - step: 5, training_loss: 2.07315e+01
2025-07-11 18:24:42,112 - step: 6, training_loss: 2.11213e+01
2025-07-11 18:24:49,017 - step: 7, training_loss: 2.01759e+01
2025-07-11 18:24:55,856 - step: 8, training_loss: 2.09371e+01
2025-07-11 18:25:02,711 - step: 9, training_loss: 2.05582e+01
2025-07-11 18:25:09,569 - step: 10, training_loss: 2.04326e+01
2025-07-11 18:25:16,395 - step: 11, training_loss: 2.06652e+01
2025-07-11 18:25:23,251 - step: 12, training_loss: 2.05882e+01
2025-07-11 18:25:30,122 - step: 13, training_loss: 2.03532e+01
2025-07-11 18:25:36,983 - step: 14, training_loss: 2.10806e+01
2025-07-11 18:25:43,828 - step: 15, training_loss: 2.16419e+01
2025-07-11 18:25:50,696 - step: 16, training_loss: 1.95789e+01
2025-07-11 18:25:57,562 - step: 17, training_loss: 2.07880e+01
2025-07-11 18:26:04,437 - step: 18, training_loss: 1.97484e+01
2025-07-11 18:26:11,345 - step: 19, training_loss: 2.09869e+01
2025-07-11 18:26:18,287 - step: 20, training_loss: 2.09457e+01
2025-07-11 18:26:25,220 - step: 21, training_loss: 1.99214e+01
2025-07-11 18:26:32,113 - step: 22, training_loss: 2.04146e+01
2025-07-11 18:26:39,106 - step: 23, training_loss: 2.07498e+01
2025-07-11 18:26:46,023 - step: 24, training_loss: 2.09088e+01
2025-07-11 18:26:53,017 - step: 25, training_loss: 2.11807e+01
2025-07-11 18:26:59,988 - step: 26, training_loss: 2.13314e+01
2025-07-11 18:27:06,890 - step: 27, training_loss: 2.13207e+01
2025-07-11 18:27:13,839 - step: 28, training_loss: 2.13869e+01
2025-07-11 18:27:20,802 - step: 29, training_loss: 2.06880e+01
2025-07-11 18:27:27,742 - step: 30, training_loss: 2.10410e+01
2025-07-11 18:27:34,658 - step: 31, training_loss: 2.11852e+01
2025-07-11 18:27:41,584 - step: 32, training_loss: 2.13636e+01
2025-07-11 18:27:48,496 - step: 33, training_loss: 2.10827e+01
2025-07-11 18:27:55,428 - step: 34, training_loss: 2.12573e+01
2025-07-11 18:28:02,363 - step: 35, training_loss: 2.07673e+01
2025-07-11 18:28:09,344 - step: 36, training_loss: 2.06423e+01
2025-07-11 18:28:16,288 - step: 37, training_loss: 1.97014e+01
2025-07-11 18:28:23,252 - step: 38, training_loss: 2.05441e+01
2025-07-11 18:28:30,189 - step: 39, training_loss: 2.09638e+01
2025-07-11 18:28:37,149 - step: 40, training_loss: 2.03707e+01
2025-07-11 18:28:44,129 - step: 41, training_loss: 2.05793e+01
2025-07-11 18:28:51,105 - step: 42, training_loss: 2.00633e+01
2025-07-11 18:28:58,089 - step: 43, training_loss: 2.13459e+01
2025-07-11 18:29:05,135 - step: 44, training_loss: 2.00445e+01
2025-07-11 18:29:12,122 - step: 45, training_loss: 2.07321e+01
2025-07-11 18:29:19,132 - step: 46, training_loss: 2.13283e+01
2025-07-11 18:29:26,138 - step: 47, training_loss: 2.05249e+01
2025-07-11 18:29:33,179 - step: 48, training_loss: 2.02180e+01
2025-07-11 18:29:40,196 - step: 49, training_loss: 2.02154e+01
2025-07-11 18:29:47,153 - step: 50, training_loss: 2.04400e+01
2025-07-11 18:29:54,135 - step: 51, training_loss: 2.08268e+01
2025-07-11 18:30:01,132 - step: 52, training_loss: 2.14269e+01
2025-07-11 18:30:08,124 - step: 53, training_loss: 2.11583e+01
2025-07-11 18:30:15,109 - step: 54, training_loss: 2.10446e+01
2025-07-11 18:30:22,108 - step: 55, training_loss: 1.98070e+01
