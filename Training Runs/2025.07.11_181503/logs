2025-07-11 18:15:03,249 - Training run started at: 2025.07.11_181503
2025-07-11 18:15:03,249 - Run directory: Training Runs/2025.07.11_181503
2025-07-11 18:15:03,284 - NCSNpp(
  (act): SiLU()
  (time_embed): GaussianFourierProjection()
  (time_mlp): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): SiLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (label_emb): Linear(in_features=1, out_features=256, bias=True)
  (input_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (down_blocks): ModuleList(
    (0-1): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
    (2): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (3-5): 3 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
  )
  (down_attn): ModuleList(
    (0-1): 2 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
    (2-5): 4 x None
  )
  (downsample): ModuleList(
    (0): Downsample(
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))
    )
    (1): Downsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    )
    (2): None
  )
  (mid_block1): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (mid_block2): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (up_blocks): ModuleList(
    (0-5): 6 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)
      (Conv_0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (6): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 192, eps=1e-06, affine=True)
      (Conv_0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (7-8): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
  )
  (up_attn): ModuleList(
    (0-5): 6 x None
    (6-8): 3 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
  )
  (upsample): ModuleList(
    (0-1): 2 x Upsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): None
  )
  (out_norm): GroupNorm(16, 64, eps=1e-06, affine=True)
  (out_act): SiLU()
  (out_conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2025-07-11 18:15:03,285 - EMA: <models.ema.ExponentialMovingAverage object at 0x30e9e4810>
2025-07-11 18:15:03,285 - Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0
)
2025-07-11 18:15:03,285 - Scaler: None.
2025-07-11 18:15:03,285 - No checkpoint found at Training Runs/2025.07.11_181503/checkpoints-meta/checkpoint.pth. Returned the same state as input
2025-07-11 18:15:03,306 - Starting training loop at step 0.
2025-07-11 18:15:03,896 - step: 0, training_loss: 1.93459e+01
2025-07-11 18:15:04,067 - step: 0, evaluation_loss: 1.98209e+01
2025-07-11 18:15:04,631 - step: 1, training_loss: 1.96649e+01
2025-07-11 18:15:05,227 - step: 2, training_loss: 2.00940e+01
2025-07-11 18:15:05,746 - step: 3, training_loss: 1.87705e+01
2025-07-11 18:15:06,310 - step: 4, training_loss: 2.37371e+01
2025-07-11 18:15:06,867 - step: 5, training_loss: 2.00567e+01
2025-07-11 18:15:07,400 - step: 6, training_loss: 2.11031e+01
2025-07-11 18:15:07,984 - step: 7, training_loss: 1.36435e+01
2025-07-11 18:15:08,571 - step: 8, training_loss: 2.07957e+01
2025-07-11 18:15:09,168 - step: 9, training_loss: 2.24473e+01
2025-07-11 18:15:09,776 - step: 10, training_loss: 2.42293e+01
2025-07-11 18:15:10,378 - step: 11, training_loss: 1.79029e+01
2025-07-11 18:15:10,988 - step: 12, training_loss: 2.18437e+01
2025-07-11 18:15:11,595 - step: 13, training_loss: 2.04344e+01
2025-07-11 18:15:12,204 - step: 14, training_loss: 2.05664e+01
2025-07-11 18:15:12,815 - step: 15, training_loss: 1.72411e+01
2025-07-11 18:15:13,429 - step: 16, training_loss: 2.11590e+01
2025-07-11 18:15:14,049 - step: 17, training_loss: 2.08538e+01
2025-07-11 18:15:14,669 - step: 18, training_loss: 2.21668e+01
2025-07-11 18:15:15,291 - step: 19, training_loss: 1.94606e+01
2025-07-11 18:15:15,932 - step: 20, training_loss: 2.02992e+01
2025-07-11 18:15:16,561 - step: 21, training_loss: 1.81445e+01
2025-07-11 18:15:17,194 - step: 22, training_loss: 2.23997e+01
2025-07-11 18:15:17,828 - step: 23, training_loss: 2.34768e+01
2025-07-11 18:15:18,469 - step: 24, training_loss: 1.95016e+01
2025-07-11 18:15:19,105 - step: 25, training_loss: 2.46570e+01
2025-07-11 18:15:19,746 - step: 26, training_loss: 1.98116e+01
2025-07-11 18:15:20,388 - step: 27, training_loss: 1.67191e+01
2025-07-11 18:15:21,054 - step: 28, training_loss: 2.30170e+01
2025-07-11 18:15:21,706 - step: 29, training_loss: 1.98957e+01
2025-07-11 18:15:22,374 - step: 30, training_loss: 2.33602e+01
2025-07-11 18:15:23,042 - step: 31, training_loss: 1.75301e+01
2025-07-11 18:15:23,707 - step: 32, training_loss: 2.49165e+01
2025-07-11 18:15:24,378 - step: 33, training_loss: 1.96382e+01
2025-07-11 18:15:25,046 - step: 34, training_loss: 2.27453e+01
2025-07-11 18:15:25,718 - step: 35, training_loss: 2.39019e+01
2025-07-11 18:15:26,395 - step: 36, training_loss: 2.22956e+01
2025-07-11 18:15:27,073 - step: 37, training_loss: 2.25809e+01
2025-07-11 18:15:27,753 - step: 38, training_loss: 2.38599e+01
2025-07-11 18:15:28,444 - step: 39, training_loss: 2.25231e+01
2025-07-11 18:15:29,160 - step: 40, training_loss: 1.99567e+01
2025-07-11 18:15:29,864 - step: 41, training_loss: 2.06262e+01
2025-07-11 18:15:30,569 - step: 42, training_loss: 2.27267e+01
2025-07-11 18:15:31,275 - step: 43, training_loss: 1.73543e+01
2025-07-11 18:15:31,986 - step: 44, training_loss: 1.98114e+01
2025-07-11 18:15:32,696 - step: 45, training_loss: 1.85043e+01
2025-07-11 18:15:33,413 - step: 46, training_loss: 2.05881e+01
2025-07-11 18:15:34,126 - step: 47, training_loss: 2.34093e+01
2025-07-11 18:15:34,846 - step: 48, training_loss: 2.32872e+01
2025-07-11 18:15:35,579 - step: 49, training_loss: 2.19505e+01
2025-07-11 18:15:36,318 - step: 50, training_loss: 1.94863e+01
2025-07-11 18:15:37,070 - step: 51, training_loss: 1.91815e+01
2025-07-11 18:15:37,824 - step: 52, training_loss: 2.27335e+01
2025-07-11 18:15:38,566 - step: 53, training_loss: 1.68871e+01
2025-07-11 18:15:39,309 - step: 54, training_loss: 1.72348e+01
2025-07-11 18:15:40,047 - step: 55, training_loss: 2.15129e+01
2025-07-11 18:15:40,798 - step: 56, training_loss: 2.22275e+01
2025-07-11 18:15:41,548 - step: 57, training_loss: 1.96277e+01
2025-07-11 18:15:42,315 - step: 58, training_loss: 2.10077e+01
2025-07-11 18:15:43,082 - step: 59, training_loss: 2.16690e+01
2025-07-11 18:15:43,840 - step: 60, training_loss: 1.82086e+01
2025-07-11 18:15:44,599 - step: 61, training_loss: 2.02527e+01
2025-07-11 18:15:45,367 - step: 62, training_loss: 1.87276e+01
2025-07-11 18:15:46,175 - step: 63, training_loss: 2.16086e+01
2025-07-11 18:15:46,957 - step: 64, training_loss: 2.20455e+01
2025-07-11 18:15:47,736 - step: 65, training_loss: 2.10305e+01
2025-07-11 18:15:48,511 - step: 66, training_loss: 2.18579e+01
2025-07-11 18:15:49,297 - step: 67, training_loss: 2.04606e+01
2025-07-11 18:15:50,086 - step: 68, training_loss: 2.20789e+01
2025-07-11 18:15:50,876 - step: 69, training_loss: 1.69117e+01
2025-07-11 18:15:51,668 - step: 70, training_loss: 2.30348e+01
2025-07-11 18:15:52,462 - step: 71, training_loss: 1.84275e+01
2025-07-11 18:15:53,262 - step: 72, training_loss: 2.16901e+01
2025-07-11 18:15:54,069 - step: 73, training_loss: 2.26519e+01
2025-07-11 18:15:54,869 - step: 74, training_loss: 1.86557e+01
2025-07-11 18:15:55,664 - step: 75, training_loss: 2.27735e+01
2025-07-11 18:15:56,475 - step: 76, training_loss: 2.09531e+01
2025-07-11 18:15:57,287 - step: 77, training_loss: 1.99164e+01
2025-07-11 18:15:58,100 - step: 78, training_loss: 1.99250e+01
2025-07-11 18:15:58,967 - step: 79, training_loss: 2.39918e+01
2025-07-11 18:15:59,802 - step: 80, training_loss: 1.96369e+01
2025-07-11 18:16:00,672 - step: 81, training_loss: 1.72905e+01
2025-07-11 18:16:01,499 - step: 82, training_loss: 2.03663e+01
2025-07-11 18:16:02,330 - step: 83, training_loss: 2.43794e+01
2025-07-11 18:16:03,216 - step: 84, training_loss: 2.19263e+01
2025-07-11 18:16:04,076 - step: 85, training_loss: 2.18776e+01
2025-07-11 18:16:04,909 - step: 86, training_loss: 2.19889e+01
2025-07-11 18:16:05,745 - step: 87, training_loss: 2.27490e+01
2025-07-11 18:16:06,594 - step: 88, training_loss: 2.60440e+01
2025-07-11 18:16:07,453 - step: 89, training_loss: 2.05491e+01
2025-07-11 18:16:08,340 - step: 90, training_loss: 2.06368e+01
2025-07-11 18:16:09,221 - step: 91, training_loss: 1.96568e+01
2025-07-11 18:16:10,073 - step: 92, training_loss: 2.27259e+01
2025-07-11 18:16:10,940 - step: 93, training_loss: 1.73206e+01
2025-07-11 18:16:11,819 - step: 94, training_loss: 2.11477e+01
2025-07-11 18:16:12,710 - step: 95, training_loss: 2.15460e+01
2025-07-11 18:16:13,581 - step: 96, training_loss: 2.34050e+01
2025-07-11 18:16:14,501 - step: 97, training_loss: 1.95342e+01
2025-07-11 18:16:15,382 - step: 98, training_loss: 1.97511e+01
2025-07-11 18:16:16,256 - step: 99, training_loss: 2.17393e+01
2025-07-11 18:16:17,162 - step: 100, training_loss: 2.27077e+01
2025-07-11 18:16:17,321 - step: 100, evaluation_loss: 2.21412e+01
2025-07-11 18:16:18,228 - step: 101, training_loss: 2.15162e+01
2025-07-11 18:16:19,137 - step: 102, training_loss: 1.79966e+01
2025-07-11 18:16:20,086 - step: 103, training_loss: 2.27825e+01
2025-07-11 18:16:20,996 - step: 104, training_loss: 1.90816e+01
2025-07-11 18:16:21,900 - step: 105, training_loss: 2.34794e+01
2025-07-11 18:16:22,803 - step: 106, training_loss: 2.06685e+01
2025-07-11 18:16:23,727 - step: 107, training_loss: 2.03889e+01
2025-07-11 18:16:24,651 - step: 108, training_loss: 2.40991e+01
2025-07-11 18:16:25,568 - step: 109, training_loss: 2.59127e+01
2025-07-11 18:16:26,511 - step: 110, training_loss: 2.10526e+01
2025-07-11 18:16:27,436 - step: 111, training_loss: 2.33494e+01
2025-07-11 18:16:28,373 - step: 112, training_loss: 2.14397e+01
2025-07-11 18:16:29,316 - step: 113, training_loss: 2.03267e+01
2025-07-11 18:16:30,245 - step: 114, training_loss: 2.02949e+01
2025-07-11 18:16:31,174 - step: 115, training_loss: 1.97885e+01
2025-07-11 18:16:32,109 - step: 116, training_loss: 2.05930e+01
2025-07-11 18:16:33,079 - step: 117, training_loss: 2.08854e+01
2025-07-11 18:16:34,065 - step: 118, training_loss: 1.87531e+01
2025-07-11 18:16:35,049 - step: 119, training_loss: 2.17172e+01
2025-07-11 18:16:36,188 - step: 120, training_loss: 2.62309e+01
2025-07-11 18:16:37,167 - step: 121, training_loss: 2.16054e+01
2025-07-11 18:16:38,116 - step: 122, training_loss: 2.36584e+01
2025-07-11 18:16:39,099 - step: 123, training_loss: 1.83031e+01
2025-07-11 18:16:40,081 - step: 124, training_loss: 2.15614e+01
2025-07-11 18:16:41,040 - step: 125, training_loss: 2.40825e+01
2025-07-11 18:16:42,031 - step: 126, training_loss: 1.93977e+01
2025-07-11 18:16:43,001 - step: 127, training_loss: 2.10847e+01
2025-07-11 18:16:43,979 - step: 128, training_loss: 2.45695e+01
2025-07-11 18:16:44,976 - step: 129, training_loss: 2.32696e+01
2025-07-11 18:16:45,951 - step: 130, training_loss: 2.37604e+01
2025-07-11 18:16:46,920 - step: 131, training_loss: 2.54482e+01
2025-07-11 18:16:47,911 - step: 132, training_loss: 2.23139e+01
2025-07-11 18:16:48,893 - step: 133, training_loss: 2.40547e+01
2025-07-11 18:16:49,885 - step: 134, training_loss: 1.89538e+01
2025-07-11 18:16:50,876 - step: 135, training_loss: 1.99827e+01
2025-07-11 18:16:51,865 - step: 136, training_loss: 1.77013e+01
2025-07-11 18:16:52,878 - step: 137, training_loss: 1.83627e+01
2025-07-11 18:16:53,878 - step: 138, training_loss: 1.75905e+01
2025-07-11 18:16:54,887 - step: 139, training_loss: 2.09459e+01
2025-07-11 18:16:55,901 - step: 140, training_loss: 2.24683e+01
2025-07-11 18:16:56,904 - step: 141, training_loss: 1.90342e+01
2025-07-11 18:16:57,903 - step: 142, training_loss: 1.81535e+01
2025-07-11 18:16:58,915 - step: 143, training_loss: 1.85452e+01
2025-07-11 18:16:59,935 - step: 144, training_loss: 2.22938e+01
2025-07-11 18:17:00,968 - step: 145, training_loss: 2.07308e+01
2025-07-11 18:17:01,993 - step: 146, training_loss: 2.31395e+01
2025-07-11 18:17:03,014 - step: 147, training_loss: 1.98144e+01
2025-07-11 18:17:04,055 - step: 148, training_loss: 1.97985e+01
2025-07-11 18:17:05,096 - step: 149, training_loss: 1.90630e+01
2025-07-11 18:17:06,134 - step: 150, training_loss: 1.75558e+01
2025-07-11 18:17:07,183 - step: 151, training_loss: 2.18133e+01
2025-07-11 18:17:08,230 - step: 152, training_loss: 2.33953e+01
2025-07-11 18:17:09,277 - step: 153, training_loss: 2.19105e+01
2025-07-11 18:17:10,346 - step: 154, training_loss: 1.89181e+01
2025-07-11 18:17:11,423 - step: 155, training_loss: 1.93106e+01
2025-07-11 18:17:12,504 - step: 156, training_loss: 2.30736e+01
2025-07-11 18:17:13,554 - step: 157, training_loss: 2.12177e+01
2025-07-11 18:17:14,600 - step: 158, training_loss: 1.80732e+01
2025-07-11 18:17:15,672 - step: 159, training_loss: 1.62629e+01
2025-07-11 18:17:16,738 - step: 160, training_loss: 1.86387e+01
2025-07-11 18:17:17,838 - step: 161, training_loss: 2.15419e+01
2025-07-11 18:17:18,927 - step: 162, training_loss: 1.78975e+01
2025-07-11 18:17:20,009 - step: 163, training_loss: 2.01952e+01
2025-07-11 18:17:21,144 - step: 164, training_loss: 2.01800e+01
2025-07-11 18:17:22,276 - step: 165, training_loss: 2.10264e+01
2025-07-11 18:17:23,389 - step: 166, training_loss: 2.38938e+01
2025-07-11 18:17:24,521 - step: 167, training_loss: 2.03258e+01
2025-07-11 18:17:25,631 - step: 168, training_loss: 1.86775e+01
2025-07-11 18:17:26,772 - step: 169, training_loss: 2.37093e+01
2025-07-11 18:17:27,916 - step: 170, training_loss: 1.78769e+01
2025-07-11 18:17:29,054 - step: 171, training_loss: 1.76948e+01
2025-07-11 18:17:30,194 - step: 172, training_loss: 1.92952e+01
2025-07-11 18:17:31,334 - step: 173, training_loss: 2.07190e+01
2025-07-11 18:17:32,622 - step: 174, training_loss: 2.00075e+01
2025-07-11 18:17:33,766 - step: 175, training_loss: 1.78550e+01
2025-07-11 18:17:34,876 - step: 176, training_loss: 2.25047e+01
2025-07-11 18:17:35,972 - step: 177, training_loss: 2.19334e+01
2025-07-11 18:17:37,081 - step: 178, training_loss: 2.24677e+01
2025-07-11 18:17:38,198 - step: 179, training_loss: 1.75397e+01
2025-07-11 18:17:39,309 - step: 180, training_loss: 1.90887e+01
2025-07-11 18:17:40,436 - step: 181, training_loss: 2.05408e+01
2025-07-11 18:17:41,560 - step: 182, training_loss: 1.97204e+01
2025-07-11 18:17:42,690 - step: 183, training_loss: 1.79192e+01
2025-07-11 18:17:43,814 - step: 184, training_loss: 1.91579e+01
2025-07-11 18:17:44,938 - step: 185, training_loss: 1.95892e+01
2025-07-11 18:17:46,067 - step: 186, training_loss: 1.81572e+01
2025-07-11 18:17:47,203 - step: 187, training_loss: 2.17344e+01
2025-07-11 18:17:48,345 - step: 188, training_loss: 2.06467e+01
2025-07-11 18:17:49,488 - step: 189, training_loss: 2.00511e+01
2025-07-11 18:17:50,628 - step: 190, training_loss: 1.72487e+01
2025-07-11 18:17:51,777 - step: 191, training_loss: 2.57020e+01
2025-07-11 18:17:52,925 - step: 192, training_loss: 1.89963e+01
2025-07-11 18:17:54,089 - step: 193, training_loss: 2.19832e+01
2025-07-11 18:17:55,241 - step: 194, training_loss: 2.14774e+01
2025-07-11 18:17:56,392 - step: 195, training_loss: 1.78829e+01
2025-07-11 18:17:57,586 - step: 196, training_loss: 2.11212e+01
2025-07-11 18:17:58,767 - step: 197, training_loss: 1.68559e+01
