2025-07-23 13:38:07,243 - Training run started at: 2025.07.23_133807
2025-07-23 13:38:07,243 - Run directory: Training Runs/2025.07.23_133807
2025-07-23 13:38:07,303 - NCSNpp(
  (act): SiLU()
  (time_embed): GaussianFourierProjection()
  (time_mlp): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): SiLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (label_emb): Linear(in_features=1, out_features=256, bias=True)
  (input_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (down_blocks): ModuleList(
    (0-1): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.2, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
    (2): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.2, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (3-5): 3 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.2, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
  )
  (down_attn): ModuleList(
    (0-1): 2 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
    (2-5): 4 x None
  )
  (downsample): ModuleList(
    (0): Downsample(
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))
    )
    (1): Downsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    )
    (2): None
  )
  (mid_block1): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.2, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (mid_block2): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.2, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (up_blocks): ModuleList(
    (0-5): 6 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)
      (Conv_0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.2, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (6): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 192, eps=1e-06, affine=True)
      (Conv_0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.2, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (7-8): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.2, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
  )
  (up_attn): ModuleList(
    (0-5): 6 x None
    (6-8): 3 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
  )
  (upsample): ModuleList(
    (0-1): 2 x Upsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): None
  )
  (out_norm): GroupNorm(16, 64, eps=1e-06, affine=True)
  (out_act): SiLU()
  (out_conv): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2025-07-23 13:38:07,304 - EMA: <models.ema.ExponentialMovingAverage object at 0x31528c490>
2025-07-23 13:38:07,304 - Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
2025-07-23 13:38:07,304 - Scaler: None.
2025-07-23 13:38:07,304 - No checkpoint found at Training Runs/2025.07.23_133807/checkpoints-meta/checkpoint.pth. Returned the same state as input
2025-07-23 13:38:07,324 - Starting training loop at step 0.
2025-07-23 13:38:07,461 - step: 0, training_loss: 1.95360e+01
2025-07-23 13:38:07,491 - step: 0, evaluation_loss: 2.64073e+01
2025-07-23 13:38:07,604 - step: 1, training_loss: 1.21604e+01
2025-07-23 13:38:07,722 - step: 2, training_loss: 8.82525e+00
2025-07-23 13:38:07,841 - step: 3, training_loss: 1.80317e+01
2025-07-23 13:38:07,963 - step: 4, training_loss: 1.50402e+01
2025-07-23 13:38:08,089 - step: 5, training_loss: 2.22553e+01
2025-07-23 13:38:08,219 - step: 6, training_loss: 1.92119e+01
2025-07-23 13:38:08,352 - step: 7, training_loss: 2.67763e+01
2025-07-23 13:38:08,493 - step: 8, training_loss: 1.81333e+01
2025-07-23 13:38:08,634 - step: 9, training_loss: 3.32292e+01
2025-07-23 13:38:08,775 - step: 10, training_loss: 1.97165e+01
2025-07-23 13:38:08,921 - step: 11, training_loss: 2.29108e+01
2025-07-23 13:38:09,071 - step: 12, training_loss: 2.38286e+01
2025-07-23 13:38:09,223 - step: 13, training_loss: 2.09487e+01
2025-07-23 13:38:09,379 - step: 14, training_loss: 2.33507e+01
2025-07-23 13:38:09,540 - step: 15, training_loss: 1.96213e+01
2025-07-23 13:38:09,702 - step: 16, training_loss: 2.00294e+01
2025-07-23 13:38:09,868 - step: 17, training_loss: 1.30601e+01
2025-07-23 13:38:10,035 - step: 18, training_loss: 1.30357e+01
2025-07-23 13:38:10,203 - step: 19, training_loss: 2.18758e+01
2025-07-23 13:38:10,378 - step: 20, training_loss: 1.33624e+01
2025-07-23 13:38:10,555 - step: 21, training_loss: 7.38081e+00
2025-07-23 13:38:10,733 - step: 22, training_loss: 1.47743e+01
2025-07-23 13:38:10,915 - step: 23, training_loss: 2.24178e+01
2025-07-23 13:38:11,101 - step: 24, training_loss: 1.89828e+01
2025-07-23 13:38:11,291 - step: 25, training_loss: 1.71869e+01
2025-07-23 13:38:11,482 - step: 26, training_loss: 1.46334e+01
2025-07-23 13:38:11,677 - step: 27, training_loss: 2.06453e+01
2025-07-23 13:38:11,877 - step: 28, training_loss: 2.77111e+01
2025-07-23 13:38:12,079 - step: 29, training_loss: 2.34201e+01
2025-07-23 13:38:12,282 - step: 30, training_loss: 1.73497e+01
2025-07-23 13:38:12,489 - step: 31, training_loss: 1.72169e+01
2025-07-23 13:38:12,700 - step: 32, training_loss: 3.12705e+01
2025-07-23 13:38:12,914 - step: 33, training_loss: 2.96423e+01
2025-07-23 13:38:13,131 - step: 34, training_loss: 2.65612e+01
2025-07-23 13:38:13,350 - step: 35, training_loss: 1.99295e+01
2025-07-23 13:38:13,571 - step: 36, training_loss: 1.65070e+01
2025-07-23 13:38:13,796 - step: 37, training_loss: 2.11359e+01
2025-07-23 13:38:14,024 - step: 38, training_loss: 1.61966e+01
2025-07-23 13:38:14,255 - step: 39, training_loss: 1.84765e+01
2025-07-23 13:38:14,487 - step: 40, training_loss: 1.05878e+01
2025-07-23 13:38:14,725 - step: 41, training_loss: 2.53389e+01
2025-07-23 13:38:14,964 - step: 42, training_loss: 2.30456e+01
2025-07-23 13:38:15,206 - step: 43, training_loss: 1.92621e+01
2025-07-23 13:38:15,453 - step: 44, training_loss: 1.65491e+01
2025-07-23 13:38:15,715 - step: 45, training_loss: 2.46155e+01
2025-07-23 13:38:15,967 - step: 46, training_loss: 2.92838e+01
2025-07-23 13:38:16,233 - step: 47, training_loss: 1.90338e+01
2025-07-23 13:38:16,491 - step: 48, training_loss: 1.79375e+01
2025-07-23 13:38:16,755 - step: 49, training_loss: 1.69194e+01
2025-07-23 13:38:17,019 - step: 50, training_loss: 1.46240e+01
2025-07-23 13:38:17,288 - step: 51, training_loss: 2.58108e+01
2025-07-23 13:38:17,558 - step: 52, training_loss: 1.51937e+01
2025-07-23 13:38:17,831 - step: 53, training_loss: 1.91309e+01
2025-07-23 13:38:18,109 - step: 54, training_loss: 2.05132e+01
2025-07-23 13:38:18,390 - step: 55, training_loss: 3.38360e+01
2025-07-23 13:38:18,672 - step: 56, training_loss: 1.62484e+01
2025-07-23 13:38:18,956 - step: 57, training_loss: 1.35274e+01
2025-07-23 13:38:19,251 - step: 58, training_loss: 8.46760e+00
2025-07-23 13:38:19,542 - step: 59, training_loss: 2.20044e+01
2025-07-23 13:38:19,836 - step: 60, training_loss: 1.36915e+01
2025-07-23 13:38:20,134 - step: 61, training_loss: 1.57540e+01
2025-07-23 13:38:20,437 - step: 62, training_loss: 2.53195e+01
2025-07-23 13:38:20,742 - step: 63, training_loss: 2.39553e+01
2025-07-23 13:38:21,048 - step: 64, training_loss: 1.86281e+01
2025-07-23 13:38:21,361 - step: 65, training_loss: 1.72419e+01
2025-07-23 13:38:21,676 - step: 66, training_loss: 1.17928e+01
2025-07-23 13:38:21,991 - step: 67, training_loss: 2.13454e+01
2025-07-23 13:38:22,310 - step: 68, training_loss: 2.50909e+01
2025-07-23 13:38:22,634 - step: 69, training_loss: 1.84773e+01
2025-07-23 13:38:22,959 - step: 70, training_loss: 2.57681e+01
2025-07-23 13:38:23,285 - step: 71, training_loss: 2.38727e+01
2025-07-23 13:38:23,614 - step: 72, training_loss: 2.14356e+01
2025-07-23 13:38:23,947 - step: 73, training_loss: 2.46789e+01
2025-07-23 13:38:24,283 - step: 74, training_loss: 1.61422e+01
2025-07-23 13:38:24,619 - step: 75, training_loss: 2.18367e+01
2025-07-23 13:38:24,960 - step: 76, training_loss: 2.27638e+01
2025-07-23 13:38:25,306 - step: 77, training_loss: 2.00396e+01
2025-07-23 13:38:25,657 - step: 78, training_loss: 1.89990e+01
2025-07-23 13:38:26,008 - step: 79, training_loss: 2.32222e+01
2025-07-23 13:38:26,363 - step: 80, training_loss: 2.09267e+01
2025-07-23 13:38:26,722 - step: 81, training_loss: 1.91123e+01
2025-07-23 13:38:27,085 - step: 82, training_loss: 2.20388e+01
2025-07-23 13:38:27,448 - step: 83, training_loss: 2.23759e+01
2025-07-23 13:38:27,812 - step: 84, training_loss: 1.79690e+01
2025-07-23 13:38:28,182 - step: 85, training_loss: 1.90973e+01
2025-07-23 13:38:28,556 - step: 86, training_loss: 2.96027e+01
2025-07-23 13:38:28,930 - step: 87, training_loss: 6.09393e+00
2025-07-23 13:38:29,304 - step: 88, training_loss: 2.49770e+01
2025-07-23 13:38:29,685 - step: 89, training_loss: 1.55679e+01
2025-07-23 13:38:30,069 - step: 90, training_loss: 2.69664e+01
2025-07-23 13:38:30,455 - step: 91, training_loss: 2.04921e+01
2025-07-23 13:38:30,845 - step: 92, training_loss: 2.52992e+01
2025-07-23 13:38:31,238 - step: 93, training_loss: 2.22420e+01
2025-07-23 13:38:31,630 - step: 94, training_loss: 2.18972e+01
2025-07-23 13:38:32,030 - step: 95, training_loss: 1.59702e+01
2025-07-23 13:38:32,439 - step: 96, training_loss: 2.66971e+01
2025-07-23 13:38:32,844 - step: 97, training_loss: 2.87035e+01
2025-07-23 13:38:33,251 - step: 98, training_loss: 2.69988e+01
2025-07-23 13:38:33,662 - step: 99, training_loss: 1.53656e+01
2025-07-23 13:38:34,076 - step: 100, training_loss: 1.62584e+01
2025-07-23 13:38:34,492 - step: 101, training_loss: 3.60886e+01
2025-07-23 13:38:34,912 - step: 102, training_loss: 8.65043e+00
2025-07-23 13:38:35,335 - step: 103, training_loss: 2.54902e+01
2025-07-23 13:38:35,762 - step: 104, training_loss: 2.42979e+01
2025-07-23 13:38:36,193 - step: 105, training_loss: 2.27710e+01
2025-07-23 13:38:36,625 - step: 106, training_loss: 2.45260e+01
2025-07-23 13:38:37,058 - step: 107, training_loss: 8.35396e+00
2025-07-23 13:38:37,494 - step: 108, training_loss: 2.03858e+01
2025-07-23 13:38:37,940 - step: 109, training_loss: 2.26860e+01
2025-07-23 13:38:38,385 - step: 110, training_loss: 2.12857e+01
2025-07-23 13:38:38,834 - step: 111, training_loss: 2.59342e+01
2025-07-23 13:38:39,288 - step: 112, training_loss: 1.17157e+01
2025-07-23 13:38:39,744 - step: 113, training_loss: 1.90222e+01
2025-07-23 13:38:40,199 - step: 114, training_loss: 1.11268e+01
2025-07-23 13:38:40,655 - step: 115, training_loss: 1.63137e+01
2025-07-23 13:38:41,118 - step: 116, training_loss: 2.57457e+01
2025-07-23 13:38:41,593 - step: 117, training_loss: 1.68201e+01
2025-07-23 13:38:42,061 - step: 118, training_loss: 2.95946e+01
2025-07-23 13:38:42,533 - step: 119, training_loss: 2.37262e+01
2025-07-23 13:38:43,004 - step: 120, training_loss: 2.54996e+01
2025-07-23 13:38:43,486 - step: 121, training_loss: 2.78921e+01
2025-07-23 13:38:43,964 - step: 122, training_loss: 1.35348e+01
2025-07-23 13:38:44,445 - step: 123, training_loss: 1.49037e+01
2025-07-23 13:38:44,933 - step: 124, training_loss: 1.14332e+01
2025-07-23 13:38:45,430 - step: 125, training_loss: 2.43864e+01
2025-07-23 13:38:45,928 - step: 126, training_loss: 1.77288e+01
2025-07-23 13:38:46,433 - step: 127, training_loss: 1.84281e+01
2025-07-23 13:38:46,919 - step: 128, training_loss: 2.32214e+01
2025-07-23 13:38:47,411 - step: 129, training_loss: 1.80858e+01
2025-07-23 13:38:47,911 - step: 130, training_loss: 2.62930e+01
2025-07-23 13:38:48,430 - step: 131, training_loss: 2.36431e+01
2025-07-23 13:38:48,945 - step: 132, training_loss: 2.59042e+01
2025-07-23 13:38:49,481 - step: 133, training_loss: 2.42638e+01
2025-07-23 13:38:49,981 - step: 134, training_loss: 1.88887e+01
2025-07-23 13:38:50,481 - step: 135, training_loss: 1.69177e+01
2025-07-23 13:38:51,003 - step: 136, training_loss: 2.52246e+01
2025-07-23 13:38:51,522 - step: 137, training_loss: 1.91263e+01
2025-07-23 13:38:52,034 - step: 138, training_loss: 3.11187e+01
2025-07-23 13:38:52,551 - step: 139, training_loss: 2.48083e+01
2025-07-23 13:38:53,076 - step: 140, training_loss: 2.46354e+01
2025-07-23 13:38:53,606 - step: 141, training_loss: 2.01620e+01
2025-07-23 13:38:54,136 - step: 142, training_loss: 1.26175e+01
2025-07-23 13:38:54,671 - step: 143, training_loss: 1.00238e+01
2025-07-23 13:38:55,222 - step: 144, training_loss: 1.42857e+01
2025-07-23 13:38:55,762 - step: 145, training_loss: 1.11897e+01
2025-07-23 13:38:56,294 - step: 146, training_loss: 1.43773e+01
2025-07-23 13:38:56,831 - step: 147, training_loss: 2.26902e+01
2025-07-23 13:38:57,383 - step: 148, training_loss: 3.03761e+01
2025-07-23 13:38:57,943 - step: 149, training_loss: 1.97287e+01
2025-07-23 13:38:58,515 - step: 150, training_loss: 2.42528e+01
2025-07-23 13:38:59,092 - step: 151, training_loss: 1.67874e+01
2025-07-23 13:38:59,667 - step: 152, training_loss: 2.32471e+01
2025-07-23 13:39:00,246 - step: 153, training_loss: 1.63262e+01
2025-07-23 13:39:00,830 - step: 154, training_loss: 2.18058e+01
2025-07-23 13:39:01,422 - step: 155, training_loss: 1.60356e+01
2025-07-23 13:39:02,007 - step: 156, training_loss: 1.99576e+01
2025-07-23 13:39:02,596 - step: 157, training_loss: 1.81197e+01
2025-07-23 13:39:03,197 - step: 158, training_loss: 2.51657e+01
2025-07-23 13:39:03,801 - step: 159, training_loss: 1.77393e+01
2025-07-23 13:39:04,414 - step: 160, training_loss: 1.36956e+01
2025-07-23 13:39:05,018 - step: 161, training_loss: 2.20019e+01
2025-07-23 13:39:05,637 - step: 162, training_loss: 2.64909e+01
2025-07-23 13:39:06,251 - step: 163, training_loss: 2.41886e+01
2025-07-23 13:39:06,869 - step: 164, training_loss: 2.86060e+01
2025-07-23 13:39:07,520 - step: 165, training_loss: 2.10151e+01
2025-07-23 13:39:08,150 - step: 166, training_loss: 2.49497e+01
2025-07-23 13:39:08,762 - step: 167, training_loss: 2.23831e+01
2025-07-23 13:39:09,540 - step: 168, training_loss: 2.64016e+01
2025-07-23 13:39:10,236 - step: 169, training_loss: 2.37047e+01
2025-07-23 13:39:10,854 - step: 170, training_loss: 2.20165e+01
2025-07-23 13:39:11,529 - step: 171, training_loss: 2.04444e+01
2025-07-23 13:39:12,142 - step: 172, training_loss: 2.13422e+01
2025-07-23 13:39:12,747 - step: 173, training_loss: 2.31528e+01
2025-07-23 13:39:13,360 - step: 174, training_loss: 1.09120e+01
2025-07-23 13:39:13,976 - step: 175, training_loss: 2.41951e+01
2025-07-23 13:39:14,608 - step: 176, training_loss: 1.97706e+01
2025-07-23 13:39:15,240 - step: 177, training_loss: 1.84526e+01
2025-07-23 13:39:15,862 - step: 178, training_loss: 2.35446e+01
2025-07-23 13:39:16,485 - step: 179, training_loss: 2.15499e+01
2025-07-23 13:39:17,117 - step: 180, training_loss: 3.07052e+01
2025-07-23 13:39:17,747 - step: 181, training_loss: 2.44946e+01
2025-07-23 13:39:18,382 - step: 182, training_loss: 8.12309e+00
2025-07-23 13:39:19,027 - step: 183, training_loss: 2.08534e+01
2025-07-23 13:39:19,670 - step: 184, training_loss: 2.67794e+01
2025-07-23 13:39:20,319 - step: 185, training_loss: 2.16683e+01
2025-07-23 13:39:20,968 - step: 186, training_loss: 2.88165e+01
2025-07-23 13:39:21,617 - step: 187, training_loss: 2.06068e+01
2025-07-23 13:39:22,273 - step: 188, training_loss: 1.70122e+01
2025-07-23 13:39:22,927 - step: 189, training_loss: 2.74413e+01
2025-07-23 13:39:23,579 - step: 190, training_loss: 2.25895e+01
2025-07-23 13:39:24,240 - step: 191, training_loss: 1.87640e+01
2025-07-23 13:39:24,904 - step: 192, training_loss: 1.87598e+01
2025-07-23 13:39:25,569 - step: 193, training_loss: 2.71222e+01
2025-07-23 13:39:26,238 - step: 194, training_loss: 2.48896e+01
2025-07-23 13:39:26,907 - step: 195, training_loss: 2.61620e+01
2025-07-23 13:39:27,587 - step: 196, training_loss: 2.42332e+01
2025-07-23 13:39:28,264 - step: 197, training_loss: 1.60131e+01
2025-07-23 13:39:28,941 - step: 198, training_loss: 2.32752e+01
2025-07-23 13:39:29,627 - step: 199, training_loss: 2.77395e+01
2025-07-23 13:39:30,318 - step: 200, training_loss: 1.98763e+01
2025-07-23 13:39:31,005 - step: 201, training_loss: 2.49174e+01
2025-07-23 13:39:31,700 - step: 202, training_loss: 2.55245e+01
2025-07-23 13:39:32,400 - step: 203, training_loss: 1.99861e+01
2025-07-23 13:39:33,095 - step: 204, training_loss: 9.11987e+00
2025-07-23 13:39:33,795 - step: 205, training_loss: 2.68273e+01
2025-07-23 13:39:34,499 - step: 206, training_loss: 1.95938e+01
2025-07-23 13:39:35,206 - step: 207, training_loss: 1.88944e+01
2025-07-23 13:39:35,917 - step: 208, training_loss: 2.49599e+01
2025-07-23 13:39:36,623 - step: 209, training_loss: 7.76975e+00
2025-07-23 13:39:37,335 - step: 210, training_loss: 2.25102e+01
2025-07-23 13:39:38,052 - step: 211, training_loss: 1.21213e+01
2025-07-23 13:39:38,771 - step: 212, training_loss: 1.72584e+01
2025-07-23 13:39:39,489 - step: 213, training_loss: 1.68928e+01
2025-07-23 13:39:40,218 - step: 214, training_loss: 2.60533e+01
2025-07-23 13:39:40,945 - step: 215, training_loss: 1.29228e+01
2025-07-23 13:39:41,679 - step: 216, training_loss: 1.70206e+01
2025-07-23 13:39:42,417 - step: 217, training_loss: 1.87580e+01
2025-07-23 13:39:43,152 - step: 218, training_loss: 2.81319e+01
2025-07-23 13:39:43,885 - step: 219, training_loss: 1.85683e+01
2025-07-23 13:39:44,626 - step: 220, training_loss: 1.82233e+01
2025-07-23 13:39:45,371 - step: 221, training_loss: 2.57252e+01
2025-07-23 13:39:46,121 - step: 222, training_loss: 2.66904e+01
2025-07-23 13:39:46,867 - step: 223, training_loss: 1.70706e+01
2025-07-23 13:39:47,620 - step: 224, training_loss: 2.31671e+01
2025-07-23 13:39:48,376 - step: 225, training_loss: 1.89259e+01
2025-07-23 13:39:49,140 - step: 226, training_loss: 2.12999e+01
2025-07-23 13:39:49,899 - step: 227, training_loss: 2.17689e+01
2025-07-23 13:39:50,670 - step: 228, training_loss: 1.77952e+01
2025-07-23 13:39:51,431 - step: 229, training_loss: 1.75827e+01
2025-07-23 13:39:52,203 - step: 230, training_loss: 1.00196e+01
2025-07-23 13:39:52,979 - step: 231, training_loss: 1.24562e+01
2025-07-23 13:39:53,754 - step: 232, training_loss: 1.20602e+01
2025-07-23 13:39:54,535 - step: 233, training_loss: 1.27834e+01
2025-07-23 13:39:55,323 - step: 234, training_loss: 2.14201e+01
2025-07-23 13:39:56,106 - step: 235, training_loss: 2.70857e+01
2025-07-23 13:39:56,895 - step: 236, training_loss: 1.72513e+01
2025-07-23 13:39:57,687 - step: 237, training_loss: 2.84299e+01
2025-07-23 13:39:58,476 - step: 238, training_loss: 1.77594e+01
2025-07-23 13:39:59,274 - step: 239, training_loss: 2.00636e+01
2025-07-23 13:40:00,083 - step: 240, training_loss: 2.82308e+01
2025-07-23 13:40:00,882 - step: 241, training_loss: 2.03951e+01
2025-07-23 13:40:01,698 - step: 242, training_loss: 9.32874e+00
2025-07-23 13:40:02,503 - step: 243, training_loss: 1.46882e+01
2025-07-23 13:40:03,314 - step: 244, training_loss: 2.18258e+01
2025-07-23 13:40:04,124 - step: 245, training_loss: 2.47963e+01
2025-07-23 13:40:04,937 - step: 246, training_loss: 1.55504e+01
2025-07-23 13:40:05,773 - step: 247, training_loss: 2.97552e+01
2025-07-23 13:40:06,590 - step: 248, training_loss: 1.34372e+01
2025-07-23 13:40:07,422 - step: 249, training_loss: 1.01795e+01
2025-07-23 13:40:08,246 - step: 250, training_loss: 3.04241e+01
2025-07-23 13:40:09,073 - step: 251, training_loss: 2.08172e+01
2025-07-23 13:40:09,905 - step: 252, training_loss: 2.17996e+01
2025-07-23 13:40:10,739 - step: 253, training_loss: 1.77692e+01
2025-07-23 13:40:11,574 - step: 254, training_loss: 1.91616e+01
2025-07-23 13:40:12,417 - step: 255, training_loss: 1.22578e+01
2025-07-23 13:40:13,262 - step: 256, training_loss: 2.03610e+01
2025-07-23 13:40:14,107 - step: 257, training_loss: 2.75810e+01
2025-07-23 13:40:14,956 - step: 258, training_loss: 1.83290e+01
2025-07-23 13:40:15,803 - step: 259, training_loss: 3.56609e+01
2025-07-23 13:40:16,663 - step: 260, training_loss: 2.68681e+01
2025-07-23 13:40:17,524 - step: 261, training_loss: 9.71115e+00
2025-07-23 13:40:18,387 - step: 262, training_loss: 1.25766e+01
2025-07-23 13:40:19,250 - step: 263, training_loss: 2.26874e+01
2025-07-23 13:40:20,114 - step: 264, training_loss: 2.69050e+01
2025-07-23 13:40:20,989 - step: 265, training_loss: 3.14174e+01
2025-07-23 13:40:21,861 - step: 266, training_loss: 2.15103e+01
2025-07-23 13:40:22,739 - step: 267, training_loss: 2.34298e+01
2025-07-23 13:40:23,616 - step: 268, training_loss: 2.26617e+01
2025-07-23 13:40:24,501 - step: 269, training_loss: 1.47421e+01
2025-07-23 13:40:25,388 - step: 270, training_loss: 1.25641e+01
2025-07-23 13:40:26,274 - step: 271, training_loss: 1.65216e+01
2025-07-23 13:40:27,169 - step: 272, training_loss: 1.56131e+01
2025-07-23 13:40:28,059 - step: 273, training_loss: 2.03586e+01
2025-07-23 13:40:28,956 - step: 274, training_loss: 2.26322e+01
2025-07-23 13:40:29,851 - step: 275, training_loss: 2.53989e+01
2025-07-23 13:40:30,749 - step: 276, training_loss: 2.49920e+01
2025-07-23 13:40:31,653 - step: 277, training_loss: 2.55020e+01
2025-07-23 13:40:32,558 - step: 278, training_loss: 3.84649e+01
2025-07-23 13:40:33,466 - step: 279, training_loss: 2.39442e+01
2025-07-23 13:40:34,378 - step: 280, training_loss: 3.75022e+01
2025-07-23 13:40:35,300 - step: 281, training_loss: 2.02841e+01
2025-07-23 13:40:36,211 - step: 282, training_loss: 1.76738e+01
2025-07-23 13:40:37,131 - step: 283, training_loss: 2.22015e+01
2025-07-23 13:40:38,059 - step: 284, training_loss: 2.60312e+01
2025-07-23 13:40:38,983 - step: 285, training_loss: 2.34032e+01
2025-07-23 13:40:39,913 - step: 286, training_loss: 1.53784e+01
2025-07-23 13:40:40,848 - step: 287, training_loss: 2.66098e+01
2025-07-23 13:40:41,783 - step: 288, training_loss: 2.55440e+01
2025-07-23 13:40:42,729 - step: 289, training_loss: 1.17731e+01
2025-07-23 13:40:43,675 - step: 290, training_loss: 1.97127e+01
2025-07-23 13:40:44,613 - step: 291, training_loss: 2.06068e+01
2025-07-23 13:40:45,558 - step: 292, training_loss: 1.59310e+01
2025-07-23 13:40:46,519 - step: 293, training_loss: 1.75691e+01
2025-07-23 13:40:47,469 - step: 294, training_loss: 1.55921e+01
2025-07-23 13:40:48,424 - step: 295, training_loss: 1.58137e+01
2025-07-23 13:40:49,386 - step: 296, training_loss: 8.82733e+00
2025-07-23 13:40:50,343 - step: 297, training_loss: 2.55013e+01
2025-07-23 13:40:51,305 - step: 298, training_loss: 2.81742e+01
2025-07-23 13:40:52,275 - step: 299, training_loss: 1.11686e+01
2025-07-23 13:40:53,238 - step: 300, training_loss: 1.92477e+01
2025-07-23 13:40:54,216 - step: 301, training_loss: 1.79364e+01
2025-07-23 13:40:55,203 - step: 302, training_loss: 2.79671e+01
2025-07-23 13:40:56,175 - step: 303, training_loss: 1.59830e+01
2025-07-23 13:40:57,157 - step: 304, training_loss: 1.67330e+01
2025-07-23 13:40:58,145 - step: 305, training_loss: 2.33712e+01
2025-07-23 13:40:59,128 - step: 306, training_loss: 2.67699e+01
2025-07-23 13:41:00,121 - step: 307, training_loss: 2.81904e+01
2025-07-23 13:41:01,122 - step: 308, training_loss: 1.85034e+01
2025-07-23 13:41:02,118 - step: 309, training_loss: 1.64611e+01
2025-07-23 13:41:03,118 - step: 310, training_loss: 2.17479e+01
2025-07-23 13:41:04,116 - step: 311, training_loss: 2.97785e+01
2025-07-23 13:41:05,128 - step: 312, training_loss: 7.28225e+00
2025-07-23 13:41:06,141 - step: 313, training_loss: 2.13182e+01
2025-07-23 13:41:07,157 - step: 314, training_loss: 1.56384e+01
2025-07-23 13:41:08,175 - step: 315, training_loss: 2.30255e+01
2025-07-23 13:41:09,193 - step: 316, training_loss: 1.77949e+01
2025-07-23 13:41:10,219 - step: 317, training_loss: 2.71952e+01
2025-07-23 13:41:11,245 - step: 318, training_loss: 1.55172e+01
2025-07-23 13:41:12,278 - step: 319, training_loss: 3.15885e+01
2025-07-23 13:41:13,304 - step: 320, training_loss: 2.61692e+01
2025-07-23 13:41:14,330 - step: 321, training_loss: 2.32286e+01
2025-07-23 13:41:15,361 - step: 322, training_loss: 1.62811e+01
2025-07-23 13:41:16,398 - step: 323, training_loss: 2.36619e+01
2025-07-23 13:41:17,439 - step: 324, training_loss: 1.19492e+01
2025-07-23 13:41:18,487 - step: 325, training_loss: 1.65742e+01
2025-07-23 13:41:19,525 - step: 326, training_loss: 1.56523e+01
2025-07-23 13:41:20,573 - step: 327, training_loss: 1.95401e+01
2025-07-23 13:41:21,618 - step: 328, training_loss: 2.69369e+01
2025-07-23 13:41:22,677 - step: 329, training_loss: 1.82808e+01
2025-07-23 13:41:23,732 - step: 330, training_loss: 1.58130e+01
2025-07-23 13:41:24,805 - step: 331, training_loss: 2.31420e+01
2025-07-23 13:41:25,875 - step: 332, training_loss: 2.86081e+01
2025-07-23 13:41:26,953 - step: 333, training_loss: 2.15215e+01
2025-07-23 13:41:28,041 - step: 334, training_loss: 1.55397e+01
2025-07-23 13:41:29,110 - step: 335, training_loss: 1.45712e+01
2025-07-23 13:41:30,195 - step: 336, training_loss: 1.66019e+01
2025-07-23 13:41:31,300 - step: 337, training_loss: 2.01214e+01
2025-07-23 13:41:32,402 - step: 338, training_loss: 2.07096e+01
2025-07-23 13:41:33,519 - step: 339, training_loss: 2.27241e+01
2025-07-23 13:41:34,628 - step: 340, training_loss: 3.28788e+01
2025-07-23 13:41:35,726 - step: 341, training_loss: 2.23285e+01
2025-07-23 13:41:36,816 - step: 342, training_loss: 2.26332e+01
2025-07-23 13:41:37,909 - step: 343, training_loss: 1.77521e+01
2025-07-23 13:41:39,004 - step: 344, training_loss: 2.23367e+01
2025-07-23 13:41:40,098 - step: 345, training_loss: 2.21998e+01
2025-07-23 13:41:41,208 - step: 346, training_loss: 2.59995e+01
2025-07-23 13:41:42,330 - step: 347, training_loss: 1.88574e+01
2025-07-23 13:41:43,446 - step: 348, training_loss: 2.52703e+01
2025-07-23 13:41:44,553 - step: 349, training_loss: 1.68711e+01
2025-07-23 13:41:45,667 - step: 350, training_loss: 1.72565e+01
2025-07-23 13:41:46,782 - step: 351, training_loss: 1.79719e+01
2025-07-23 13:41:47,909 - step: 352, training_loss: 1.41437e+01
2025-07-23 13:41:49,031 - step: 353, training_loss: 1.92266e+01
2025-07-23 13:41:50,162 - step: 354, training_loss: 2.17713e+01
2025-07-23 13:41:51,295 - step: 355, training_loss: 2.68908e+01
2025-07-23 13:41:52,432 - step: 356, training_loss: 1.92208e+01
2025-07-23 13:41:53,564 - step: 357, training_loss: 2.38007e+01
2025-07-23 13:41:54,708 - step: 358, training_loss: 2.56209e+01
2025-07-23 13:41:55,846 - step: 359, training_loss: 1.39752e+01
2025-07-23 13:41:56,992 - step: 360, training_loss: 1.84875e+01
2025-07-23 13:41:58,155 - step: 361, training_loss: 2.49180e+01
2025-07-23 13:41:59,327 - step: 362, training_loss: 1.69005e+01
2025-07-23 13:42:00,495 - step: 363, training_loss: 1.37389e+01
2025-07-23 13:42:01,657 - step: 364, training_loss: 1.52520e+01
2025-07-23 13:42:02,817 - step: 365, training_loss: 2.74487e+01
2025-07-23 13:42:03,968 - step: 366, training_loss: 2.62383e+01
2025-07-23 13:42:05,132 - step: 367, training_loss: 1.81087e+01
2025-07-23 13:42:06,300 - step: 368, training_loss: 1.59180e+01
2025-07-23 13:42:07,470 - step: 369, training_loss: 1.79223e+01
2025-07-23 13:42:08,646 - step: 370, training_loss: 1.39895e+01
2025-07-23 13:42:09,818 - step: 371, training_loss: 2.19347e+01
2025-07-23 13:42:10,998 - step: 372, training_loss: 1.76901e+01
2025-07-23 13:42:12,177 - step: 373, training_loss: 2.05001e+01
2025-07-23 13:42:13,370 - step: 374, training_loss: 1.30717e+01
2025-07-23 13:42:14,551 - step: 375, training_loss: 1.99027e+01
2025-07-23 13:42:15,740 - step: 376, training_loss: 2.19687e+01
2025-07-23 13:42:16,935 - step: 377, training_loss: 3.08663e+01
2025-07-23 13:42:18,135 - step: 378, training_loss: 3.11441e+01
2025-07-23 13:42:19,337 - step: 379, training_loss: 2.30930e+01
2025-07-23 13:42:20,543 - step: 380, training_loss: 2.97139e+01
2025-07-23 13:42:21,750 - step: 381, training_loss: 2.16894e+01
2025-07-23 13:42:22,961 - step: 382, training_loss: 2.37872e+01
2025-07-23 13:42:24,184 - step: 383, training_loss: 2.30875e+01
2025-07-23 13:42:25,399 - step: 384, training_loss: 1.75284e+01
2025-07-23 13:42:26,619 - step: 385, training_loss: 1.50314e+01
2025-07-23 13:42:27,845 - step: 386, training_loss: 1.77903e+01
2025-07-23 13:42:29,057 - step: 387, training_loss: 1.70868e+01
2025-07-23 13:42:30,284 - step: 388, training_loss: 1.33758e+01
2025-07-23 13:42:31,518 - step: 389, training_loss: 2.05408e+01
2025-07-23 13:42:32,750 - step: 390, training_loss: 1.51765e+01
2025-07-23 13:42:33,979 - step: 391, training_loss: 2.66078e+01
2025-07-23 13:42:35,220 - step: 392, training_loss: 2.06021e+01
2025-07-23 13:42:36,462 - step: 393, training_loss: 1.20377e+01
2025-07-23 13:42:37,705 - step: 394, training_loss: 2.57955e+01
2025-07-23 13:42:38,954 - step: 395, training_loss: 2.14568e+01
2025-07-23 13:42:40,196 - step: 396, training_loss: 2.36691e+01
2025-07-23 13:42:41,440 - step: 397, training_loss: 2.31350e+01
2025-07-23 13:42:42,697 - step: 398, training_loss: 2.35987e+01
2025-07-23 13:42:43,946 - step: 399, training_loss: 1.82618e+01
2025-07-23 13:42:45,200 - step: 400, training_loss: 2.09824e+01
2025-07-23 13:42:45,228 - step: 400, evaluation_loss: 2.37084e+01
2025-07-23 13:42:46,482 - step: 401, training_loss: 1.59927e+01
2025-07-23 13:42:47,744 - step: 402, training_loss: 3.12563e+01
2025-07-23 13:42:49,011 - step: 403, training_loss: 2.73894e+01
2025-07-23 13:42:50,281 - step: 404, training_loss: 2.82622e+01
2025-07-23 13:42:51,564 - step: 405, training_loss: 2.59441e+01
2025-07-23 13:42:52,842 - step: 406, training_loss: 1.51153e+01
2025-07-23 13:42:54,122 - step: 407, training_loss: 1.59246e+01
2025-07-23 13:42:55,405 - step: 408, training_loss: 1.61309e+01
2025-07-23 13:42:56,690 - step: 409, training_loss: 2.10064e+01
2025-07-23 13:42:57,975 - step: 410, training_loss: 2.44102e+01
2025-07-23 13:42:59,261 - step: 411, training_loss: 1.86703e+01
2025-07-23 13:43:00,559 - step: 412, training_loss: 1.92833e+01
2025-07-23 13:43:01,858 - step: 413, training_loss: 1.92031e+01
2025-07-23 13:43:03,157 - step: 414, training_loss: 1.77135e+01
2025-07-23 13:43:04,488 - step: 415, training_loss: 1.12960e+01
2025-07-23 13:43:05,799 - step: 416, training_loss: 1.80581e+01
2025-07-23 13:43:07,110 - step: 417, training_loss: 2.38658e+01
2025-07-23 13:43:08,417 - step: 418, training_loss: 1.97400e+01
2025-07-23 13:43:09,741 - step: 419, training_loss: 2.23528e+01
2025-07-23 13:43:11,064 - step: 420, training_loss: 1.86473e+01
2025-07-23 13:43:12,385 - step: 421, training_loss: 1.55082e+01
2025-07-23 13:43:13,708 - step: 422, training_loss: 2.75149e+01
2025-07-23 13:43:15,036 - step: 423, training_loss: 2.01512e+01
2025-07-23 13:43:16,367 - step: 424, training_loss: 2.70279e+01
2025-07-23 13:43:17,706 - step: 425, training_loss: 1.31548e+01
2025-07-23 13:43:19,038 - step: 426, training_loss: 1.22650e+01
2025-07-23 13:43:20,374 - step: 427, training_loss: 1.01234e+01
2025-07-23 13:43:21,726 - step: 428, training_loss: 6.43993e+01
2025-07-23 13:43:23,068 - step: 429, training_loss: 2.18337e+01
2025-07-23 13:43:24,417 - step: 430, training_loss: 1.60826e+01
2025-07-23 13:43:25,767 - step: 431, training_loss: 2.86039e+01
2025-07-23 13:43:27,101 - step: 432, training_loss: 1.74236e+01
2025-07-23 13:43:28,462 - step: 433, training_loss: 1.97413e+01
2025-07-23 13:43:29,825 - step: 434, training_loss: 2.06847e+01
2025-07-23 13:43:31,189 - step: 435, training_loss: 2.49306e+01
2025-07-23 13:43:32,558 - step: 436, training_loss: 9.84875e+00
2025-07-23 13:43:33,927 - step: 437, training_loss: 1.00618e+01
2025-07-23 13:43:35,305 - step: 438, training_loss: 2.82309e+01
2025-07-23 13:43:36,682 - step: 439, training_loss: 2.22963e+01
2025-07-23 13:43:38,062 - step: 440, training_loss: 1.16768e+01
2025-07-23 13:43:39,450 - step: 441, training_loss: 1.17271e+01
2025-07-23 13:43:40,848 - step: 442, training_loss: 1.83694e+01
2025-07-23 13:43:42,241 - step: 443, training_loss: 1.30906e+01
2025-07-23 13:43:43,649 - step: 444, training_loss: 1.56662e+01
2025-07-23 13:43:45,094 - step: 445, training_loss: 2.62826e+01
