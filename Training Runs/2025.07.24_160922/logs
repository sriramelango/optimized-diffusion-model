2025-07-24 16:09:22,517 - Training run started at: 2025.07.24_160922
2025-07-24 16:09:22,517 - Run directory: Training Runs/2025.07.24_160922
2025-07-24 16:09:22,552 - Unet1D(
  (init_conv): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))
  (time_mlp): Sequential(
    (0): SinusoidalPosEmb()
    (1): Linear(in_features=64, out_features=256, bias=True)
    (2): GELU(approximate='none')
    (3): Linear(in_features=256, out_features=256, bias=True)
  )
  (classes_mlp): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (downs): ModuleList(
    (0): ModuleList(
      (0-1): 2 x ResnetBlock(
        (mlp): Sequential(
          (0): SiLU()
          (1): Linear(in_features=320, out_features=128, bias=True)
        )
        (block1): Block(
          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (block2): Block(
          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (res_conv): Identity()
      )
      (2): Residual(
        (fn): PreNorm(
          (fn): LinearAttention(
            (to_qkv): Conv1d(64, 384, kernel_size=(1,), stride=(1,), bias=False)
            (to_out): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
          )
          (norm): RMSNorm()
        )
      )
      (3): Conv1d(64, 64, kernel_size=(4,), stride=(2,), padding=(1,))
    )
    (1): ModuleList(
      (0-1): 2 x ResnetBlock(
        (mlp): Sequential(
          (0): SiLU()
          (1): Linear(in_features=320, out_features=128, bias=True)
        )
        (block1): Block(
          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (block2): Block(
          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (res_conv): Identity()
      )
      (2): Residual(
        (fn): PreNorm(
          (fn): LinearAttention(
            (to_qkv): Conv1d(64, 384, kernel_size=(1,), stride=(1,), bias=False)
            (to_out): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
          )
          (norm): RMSNorm()
        )
      )
      (3): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))
    )
    (2): ModuleList(
      (0-1): 2 x ResnetBlock(
        (mlp): Sequential(
          (0): SiLU()
          (1): Linear(in_features=320, out_features=256, bias=True)
        )
        (block1): Block(
          (proj): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (block2): Block(
          (proj): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (res_conv): Identity()
      )
      (2): Residual(
        (fn): PreNorm(
          (fn): LinearAttention(
            (to_qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,), bias=False)
            (to_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
          (norm): RMSNorm()
        )
      )
      (3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    )
  )
  (ups): ModuleList(
    (0): ModuleList(
      (0-1): 2 x ResnetBlock(
        (mlp): Sequential(
          (0): SiLU()
          (1): Linear(in_features=320, out_features=512, bias=True)
        )
        (block1): Block(
          (proj): Conv1d(384, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (block2): Block(
          (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 256, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (res_conv): Conv1d(384, 256, kernel_size=(1,), stride=(1,))
      )
      (2): Residual(
        (fn): PreNorm(
          (fn): LinearAttention(
            (to_qkv): Conv1d(256, 384, kernel_size=(1,), stride=(1,), bias=False)
            (to_out): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
          )
          (norm): RMSNorm()
        )
      )
      (3): Sequential(
        (0): Upsample(size=33, mode='nearest')
        (1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (1): ModuleList(
      (0-1): 2 x ResnetBlock(
        (mlp): Sequential(
          (0): SiLU()
          (1): Linear(in_features=320, out_features=256, bias=True)
        )
        (block1): Block(
          (proj): Conv1d(192, 128, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (block2): Block(
          (proj): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 128, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (res_conv): Conv1d(192, 128, kernel_size=(1,), stride=(1,))
      )
      (2): Residual(
        (fn): PreNorm(
          (fn): LinearAttention(
            (to_qkv): Conv1d(128, 384, kernel_size=(1,), stride=(1,), bias=False)
            (to_out): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          )
          (norm): RMSNorm()
        )
      )
      (3): Sequential(
        (0): Upsample(size=67, mode='nearest')
        (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      )
    )
    (2): ModuleList(
      (0-1): 2 x ResnetBlock(
        (mlp): Sequential(
          (0): SiLU()
          (1): Linear(in_features=320, out_features=128, bias=True)
        )
        (block1): Block(
          (proj): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (block2): Block(
          (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
          (act): SiLU()
        )
        (res_conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
      )
      (2): Residual(
        (fn): PreNorm(
          (fn): LinearAttention(
            (to_qkv): Conv1d(64, 384, kernel_size=(1,), stride=(1,), bias=False)
            (to_out): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
          )
          (norm): RMSNorm()
        )
      )
      (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    )
  )
  (mid_block1): ResnetBlock(
    (mlp): Sequential(
      (0): SiLU()
      (1): Linear(in_features=320, out_features=512, bias=True)
    )
    (block1): Block(
      (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm): GroupNorm(4, 256, eps=1e-05, affine=True)
      (act): SiLU()
    )
    (block2): Block(
      (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm): GroupNorm(4, 256, eps=1e-05, affine=True)
      (act): SiLU()
    )
    (res_conv): Identity()
  )
  (mid_attn): Residual(
    (fn): PreNorm(
      (fn): Attention(
        (to_qkv): Conv1d(256, 384, kernel_size=(1,), stride=(1,), bias=False)
        (to_out): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
      )
      (norm): RMSNorm()
    )
  )
  (mid_block2): ResnetBlock(
    (mlp): Sequential(
      (0): SiLU()
      (1): Linear(in_features=320, out_features=512, bias=True)
    )
    (block1): Block(
      (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm): GroupNorm(4, 256, eps=1e-05, affine=True)
      (act): SiLU()
    )
    (block2): Block(
      (proj): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm): GroupNorm(4, 256, eps=1e-05, affine=True)
      (act): SiLU()
    )
    (res_conv): Identity()
  )
  (final_res_block): ResnetBlock(
    (mlp): Sequential(
      (0): SiLU()
      (1): Linear(in_features=320, out_features=128, bias=True)
    )
    (block1): Block(
      (proj): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
      (act): SiLU()
    )
    (block2): Block(
      (proj): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm): GroupNorm(4, 64, eps=1e-05, affine=True)
      (act): SiLU()
    )
    (res_conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,))
  )
  (final_conv): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
)
2025-07-24 16:09:22,553 - EMA: <models.ema.ExponentialMovingAverage object at 0x3386953d0>
2025-07-24 16:09:22,553 - Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0
)
2025-07-24 16:09:22,553 - Scaler: None.
2025-07-24 16:09:22,553 - No checkpoint found at Training Runs/2025.07.24_160922/checkpoints-meta/checkpoint.pth. Returned the same state as input
2025-07-24 16:09:22,574 - Starting training loop at step 0.
2025-07-24 16:09:29,891 - step: 0, training_loss: 5.49165e+01
2025-07-24 16:11:07,540 - step: 0, evaluation_loss: 6.43733e+01
2025-07-24 16:11:13,746 - step: 1, training_loss: 6.09373e+01
2025-07-24 16:11:19,836 - step: 2, training_loss: 7.03735e+01
2025-07-24 16:11:25,977 - step: 3, training_loss: 5.86583e+01
2025-07-24 16:11:32,659 - step: 4, training_loss: 6.51767e+01
2025-07-24 16:11:39,589 - step: 5, training_loss: 5.86239e+01
2025-07-24 16:11:47,590 - step: 6, training_loss: 6.01888e+01
2025-07-24 16:11:54,956 - step: 7, training_loss: 7.64245e+01
2025-07-24 16:12:01,760 - step: 8, training_loss: 7.66528e+01
2025-07-24 16:12:08,545 - step: 9, training_loss: 5.79386e+01
2025-07-24 16:12:14,908 - step: 10, training_loss: 6.46314e+01
2025-07-24 16:12:21,148 - step: 11, training_loss: 7.78874e+01
2025-07-24 16:12:27,378 - step: 12, training_loss: 7.07272e+01
2025-07-24 16:12:33,617 - step: 13, training_loss: 6.03642e+01
2025-07-24 16:12:39,909 - step: 14, training_loss: 5.84214e+01
2025-07-24 16:12:46,176 - step: 15, training_loss: 6.43137e+01
