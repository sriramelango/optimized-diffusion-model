2025-07-11 18:51:54,881 - Training run started at: 2025.07.11_185154
2025-07-11 18:51:54,881 - Run directory: Training Runs/2025.07.11_185154
2025-07-11 18:51:56,242 - NCSNpp(
  (act): SiLU()
  (time_embed): GaussianFourierProjection()
  (time_mlp): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): SiLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (label_emb): Linear(in_features=1, out_features=256, bias=True)
  (input_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (down_blocks): ModuleList(
    (0-1): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
    (2): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (3-5): 3 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
  )
  (down_attn): ModuleList(
    (0-1): 2 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
    (2-5): 4 x None
  )
  (downsample): ModuleList(
    (0): Downsample(
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))
    )
    (1): Downsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    )
    (2): None
  )
  (mid_block1): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (mid_block2): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (up_blocks): ModuleList(
    (0-5): 6 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)
      (Conv_0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (6): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 192, eps=1e-06, affine=True)
      (Conv_0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (7-8): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
  )
  (up_attn): ModuleList(
    (0-5): 6 x None
    (6-8): 3 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
  )
  (upsample): ModuleList(
    (0-1): 2 x Upsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): None
  )
  (out_norm): GroupNorm(16, 64, eps=1e-06, affine=True)
  (out_act): SiLU()
  (out_conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2025-07-11 18:51:56,244 - EMA: <models.ema.ExponentialMovingAverage object at 0x154c80dfa170>
2025-07-11 18:51:56,244 - Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0004
    maximize: False
    weight_decay: 0
)
2025-07-11 18:51:56,244 - Scaler: None.
2025-07-11 18:51:56,244 - No checkpoint found at Training Runs/2025.07.11_185154/checkpoints-meta/checkpoint.pth. Returned the same state as input
2025-07-11 18:51:56,290 - Starting training loop at step 0.
2025-07-11 18:51:59,314 - step: 0, training_loss: 2.09213e+01
2025-07-11 18:51:59,467 - step: 0, evaluation_loss: 2.01337e+01
2025-07-11 18:51:59,933 - step: 1, training_loss: 2.13250e+01
2025-07-11 18:52:00,450 - step: 2, training_loss: 2.10841e+01
2025-07-11 18:52:00,964 - step: 3, training_loss: 2.12818e+01
2025-07-11 18:52:01,510 - step: 4, training_loss: 2.10160e+01
2025-07-11 18:52:02,046 - step: 5, training_loss: 1.92804e+01
2025-07-11 18:52:02,602 - step: 6, training_loss: 2.06627e+01
2025-07-11 18:52:03,336 - step: 7, training_loss: 2.05974e+01
2025-07-11 18:52:03,926 - step: 8, training_loss: 2.04352e+01
2025-07-11 18:52:04,480 - step: 9, training_loss: 2.03766e+01
2025-07-11 18:52:05,070 - step: 10, training_loss: 2.11869e+01
2025-07-11 18:52:05,645 - step: 11, training_loss: 2.04095e+01
2025-07-11 18:52:06,265 - step: 12, training_loss: 2.11004e+01
2025-07-11 18:52:07,172 - step: 13, training_loss: 2.13567e+01
2025-07-11 18:52:08,193 - step: 14, training_loss: 2.07451e+01
2025-07-11 18:52:08,785 - step: 15, training_loss: 2.03911e+01
2025-07-11 18:52:09,384 - step: 16, training_loss: 2.09512e+01
2025-07-11 18:52:09,992 - step: 17, training_loss: 2.07699e+01
2025-07-11 18:52:10,640 - step: 18, training_loss: 2.03557e+01
2025-07-11 18:52:11,257 - step: 19, training_loss: 2.05695e+01
2025-07-11 18:52:11,925 - step: 20, training_loss: 2.10264e+01
2025-07-11 18:52:12,901 - step: 21, training_loss: 2.10258e+01
2025-07-11 18:52:13,548 - step: 22, training_loss: 2.15252e+01
2025-07-11 18:52:14,212 - step: 23, training_loss: 2.13972e+01
2025-07-11 18:52:14,872 - step: 24, training_loss: 2.06492e+01
2025-07-11 18:52:15,536 - step: 25, training_loss: 2.06807e+01
2025-07-11 18:52:16,201 - step: 26, training_loss: 2.07938e+01
2025-07-11 18:52:16,871 - step: 27, training_loss: 2.18433e+01
2025-07-11 18:52:17,590 - step: 28, training_loss: 2.03391e+01
2025-07-11 18:52:18,281 - step: 29, training_loss: 2.08169e+01
2025-07-11 18:52:18,976 - step: 30, training_loss: 2.04311e+01
2025-07-11 18:52:19,671 - step: 31, training_loss: 2.07982e+01
2025-07-11 18:52:20,376 - step: 32, training_loss: 2.13660e+01
2025-07-11 18:52:21,094 - step: 33, training_loss: 2.03239e+01
2025-07-11 18:52:21,825 - step: 34, training_loss: 2.14977e+01
2025-07-11 18:52:22,566 - step: 35, training_loss: 2.03083e+01
2025-07-11 18:52:23,324 - step: 36, training_loss: 2.11290e+01
2025-07-11 18:52:24,077 - step: 37, training_loss: 2.01160e+01
2025-07-11 18:52:24,846 - step: 38, training_loss: 2.07905e+01
2025-07-11 18:52:26,115 - step: 39, training_loss: 1.99225e+01
2025-07-11 18:52:26,869 - step: 40, training_loss: 2.03313e+01
2025-07-11 18:52:27,634 - step: 41, training_loss: 2.07743e+01
2025-07-11 18:52:28,429 - step: 42, training_loss: 2.01093e+01
2025-07-11 18:52:29,724 - step: 43, training_loss: 2.10660e+01
2025-07-11 18:52:30,504 - step: 44, training_loss: 1.99605e+01
2025-07-11 18:52:31,295 - step: 45, training_loss: 2.09777e+01
2025-07-11 18:52:32,090 - step: 46, training_loss: 2.19193e+01
2025-07-11 18:52:32,905 - step: 47, training_loss: 2.12322e+01
2025-07-11 18:52:33,862 - step: 48, training_loss: 2.00488e+01
2025-07-11 18:52:34,696 - step: 49, training_loss: 2.06985e+01
2025-07-11 18:52:35,531 - step: 50, training_loss: 2.00732e+01
2025-07-11 18:52:36,363 - step: 51, training_loss: 2.04049e+01
2025-07-11 18:52:37,620 - step: 52, training_loss: 2.04264e+01
2025-07-11 18:52:38,646 - step: 53, training_loss: 2.12765e+01
2025-07-11 18:52:39,503 - step: 54, training_loss: 2.07446e+01
2025-07-11 18:52:40,358 - step: 55, training_loss: 2.09772e+01
2025-07-11 18:52:41,220 - step: 56, training_loss: 1.98347e+01
2025-07-11 18:52:42,150 - step: 57, training_loss: 2.05742e+01
2025-07-11 18:52:43,198 - step: 58, training_loss: 2.06771e+01
2025-07-11 18:52:44,107 - step: 59, training_loss: 2.05624e+01
2025-07-11 18:52:44,995 - step: 60, training_loss: 2.06479e+01
2025-07-11 18:52:45,899 - step: 61, training_loss: 2.05636e+01
2025-07-11 18:52:46,802 - step: 62, training_loss: 2.09871e+01
2025-07-11 18:52:47,733 - step: 63, training_loss: 2.11572e+01
2025-07-11 18:52:48,652 - step: 64, training_loss: 2.07489e+01
2025-07-11 18:52:49,587 - step: 65, training_loss: 2.08362e+01
2025-07-11 18:52:50,523 - step: 66, training_loss: 1.98358e+01
2025-07-11 18:52:51,460 - step: 67, training_loss: 2.04767e+01
2025-07-11 18:52:52,403 - step: 68, training_loss: 2.04433e+01
2025-07-11 18:52:53,354 - step: 69, training_loss: 2.11454e+01
2025-07-11 18:52:54,307 - step: 70, training_loss: 2.05036e+01
2025-07-11 18:52:55,352 - step: 71, training_loss: 2.09839e+01
2025-07-11 18:52:56,699 - step: 72, training_loss: 2.04590e+01
2025-07-11 18:52:57,686 - step: 73, training_loss: 1.96868e+01
2025-07-11 18:52:58,666 - step: 74, training_loss: 2.11080e+01
2025-07-11 18:53:00,019 - step: 75, training_loss: 1.99064e+01
2025-07-11 18:53:01,014 - step: 76, training_loss: 2.11554e+01
2025-07-11 18:53:02,015 - step: 77, training_loss: 2.00451e+01
2025-07-11 18:53:03,039 - step: 78, training_loss: 2.03450e+01
2025-07-11 18:53:04,134 - step: 79, training_loss: 2.02817e+01
2025-07-11 18:53:05,152 - step: 80, training_loss: 2.11317e+01
2025-07-11 18:53:06,221 - step: 81, training_loss: 2.09277e+01
2025-07-11 18:53:07,738 - step: 82, training_loss: 2.08649e+01
2025-07-11 18:53:08,883 - step: 83, training_loss: 2.13459e+01
2025-07-11 18:53:09,940 - step: 84, training_loss: 2.09121e+01
