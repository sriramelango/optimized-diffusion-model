2025-07-07 17:57:58,118 - NCSNpp(
  (act): SiLU()
  (time_embed): GaussianFourierProjection()
  (time_mlp): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): SiLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (label_emb): Linear(in_features=1, out_features=256, bias=True)
  (input_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (down_blocks): ModuleList(
    (0-1): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
    (2): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Conv_0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (3-5): 3 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (act): SiLU()
    )
  )
  (down_attn): ModuleList(
    (0-1): 2 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
    (2-5): 4 x None
  )
  (downsample): ModuleList(
    (0): Downsample(
      (Conv_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))
    )
    (1): Downsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
    )
    (2): None
  )
  (mid_block1): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (mid_block2): ResnetBlockDDPMpp(
    (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (Dense_0): Linear(in_features=256, out_features=128, bias=True)
    (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
    (Dropout_0): Dropout(p=0.1, inplace=False)
    (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): SiLU()
  )
  (up_blocks): ModuleList(
    (0-5): 6 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 256, eps=1e-06, affine=True)
      (Conv_0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=128, bias=True)
      (GroupNorm_1): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (6): ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 192, eps=1e-06, affine=True)
      (Conv_0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
    (7-8): 2 x ResnetBlockDDPMpp(
      (GroupNorm_0): GroupNorm(32, 128, eps=1e-06, affine=True)
      (Conv_0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (Dense_0): Linear(in_features=256, out_features=64, bias=True)
      (GroupNorm_1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (Dropout_0): Dropout(p=0.1, inplace=False)
      (Conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (NIN_0): NIN()
      (act): SiLU()
    )
  )
  (up_attn): ModuleList(
    (0-5): 6 x None
    (6-8): 3 x AttnBlockpp(
      (GroupNorm_0): GroupNorm(16, 64, eps=1e-06, affine=True)
      (NIN_0): NIN()
      (NIN_1): NIN()
      (NIN_2): NIN()
      (NIN_3): NIN()
    )
  )
  (upsample): ModuleList(
    (0-1): 2 x Upsample(
      (Conv_0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): None
  )
  (out_norm): GroupNorm(16, 64, eps=1e-06, affine=True)
  (out_act): SiLU()
  (out_conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2025-07-07 17:57:58,119 - EMA: <models.ema.ExponentialMovingAverage object at 0x177af3c50>
2025-07-07 17:57:58,119 - Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 2e-05
    maximize: False
    weight_decay: 0
)
2025-07-07 17:57:58,119 - Scaler: None.
2025-07-07 17:57:58,120 - No checkpoint found at runs/GTOHaloImage/2025.07.07/175758/checkpoints-meta/checkpoint.pth. Returned the same state as input
2025-07-07 17:57:58,146 - Starting training loop at step 0.
2025-07-07 17:58:00,055 - step: 0, training_loss: 1.75318e+01
2025-07-07 17:58:00,462 - step: 0, evaluation_loss: 1.67319e+01
2025-07-07 17:58:02,355 - step: 1, training_loss: 1.75111e+01
2025-07-07 17:58:04,224 - step: 2, training_loss: 1.97801e+01
2025-07-07 17:58:06,074 - step: 3, training_loss: 1.80504e+01
2025-07-07 17:58:08,159 - step: 4, training_loss: 1.95882e+01
2025-07-07 17:58:10,087 - step: 5, training_loss: 2.03003e+01
2025-07-07 17:58:11,981 - step: 6, training_loss: 1.75014e+01
2025-07-07 17:58:13,922 - step: 7, training_loss: 1.89758e+01
2025-07-07 17:58:15,777 - step: 8, training_loss: 1.86631e+01
2025-07-07 17:58:17,675 - step: 9, training_loss: 1.83696e+01
2025-07-07 17:58:19,573 - step: 10, training_loss: 1.89025e+01
2025-07-07 17:58:21,460 - step: 11, training_loss: 1.89078e+01
2025-07-07 17:58:23,358 - step: 12, training_loss: 1.91211e+01
2025-07-07 17:58:25,254 - step: 13, training_loss: 1.87065e+01
2025-07-07 17:58:27,203 - step: 14, training_loss: 1.78066e+01
2025-07-07 17:58:29,075 - step: 15, training_loss: 1.84998e+01
2025-07-07 17:58:30,994 - step: 16, training_loss: 1.77145e+01
2025-07-07 17:58:32,909 - step: 17, training_loss: 1.89910e+01
2025-07-07 17:58:34,840 - step: 18, training_loss: 1.77974e+01
2025-07-07 17:58:36,764 - step: 19, training_loss: 1.90444e+01
2025-07-07 17:58:38,676 - step: 20, training_loss: 1.77302e+01
2025-07-07 17:58:40,588 - step: 21, training_loss: 1.80591e+01
2025-07-07 17:58:42,541 - step: 22, training_loss: 1.71559e+01
2025-07-07 17:58:44,492 - step: 23, training_loss: 1.99501e+01
2025-07-07 17:58:46,437 - step: 24, training_loss: 1.75656e+01
2025-07-07 17:58:48,403 - step: 25, training_loss: 1.72065e+01
2025-07-07 17:58:50,347 - step: 26, training_loss: 1.74905e+01
2025-07-07 17:58:52,306 - step: 27, training_loss: 1.73788e+01
2025-07-07 17:58:54,272 - step: 28, training_loss: 1.86157e+01
2025-07-07 17:58:56,216 - step: 29, training_loss: 1.76599e+01
2025-07-07 17:58:58,187 - step: 30, training_loss: 1.84456e+01
2025-07-07 17:59:00,172 - step: 31, training_loss: 1.86628e+01
2025-07-07 17:59:02,155 - step: 32, training_loss: 1.89469e+01
2025-07-07 17:59:04,140 - step: 33, training_loss: 1.87954e+01
2025-07-07 17:59:06,151 - step: 34, training_loss: 1.65387e+01
2025-07-07 17:59:08,137 - step: 35, training_loss: 1.65479e+01
2025-07-07 17:59:10,119 - step: 36, training_loss: 1.68773e+01
2025-07-07 17:59:12,106 - step: 37, training_loss: 1.84176e+01
2025-07-07 17:59:14,096 - step: 38, training_loss: 1.84533e+01
2025-07-07 17:59:16,131 - step: 39, training_loss: 1.79730e+01
2025-07-07 17:59:18,142 - step: 40, training_loss: 1.86145e+01
2025-07-07 17:59:20,169 - step: 41, training_loss: 1.67476e+01
2025-07-07 17:59:22,179 - step: 42, training_loss: 1.98529e+01
2025-07-07 17:59:24,175 - step: 43, training_loss: 1.62454e+01
2025-07-07 17:59:26,228 - step: 44, training_loss: 1.84032e+01
2025-07-07 17:59:28,228 - step: 45, training_loss: 1.94463e+01
2025-07-07 17:59:30,250 - step: 46, training_loss: 1.87529e+01
2025-07-07 17:59:32,286 - step: 47, training_loss: 1.74269e+01
2025-07-07 17:59:34,323 - step: 48, training_loss: 1.57099e+01
2025-07-07 17:59:36,349 - step: 49, training_loss: 1.75430e+01
2025-07-07 17:59:38,393 - step: 50, training_loss: 2.00766e+01
2025-07-07 17:59:40,420 - step: 51, training_loss: 1.97675e+01
2025-07-07 17:59:42,469 - step: 52, training_loss: 1.76730e+01
2025-07-07 17:59:44,504 - step: 53, training_loss: 1.66050e+01
2025-07-07 17:59:46,537 - step: 54, training_loss: 1.84848e+01
2025-07-07 17:59:48,611 - step: 55, training_loss: 1.78233e+01
2025-07-07 17:59:50,672 - step: 56, training_loss: 1.78527e+01
2025-07-07 17:59:52,740 - step: 57, training_loss: 1.84534e+01
2025-07-07 17:59:54,860 - step: 58, training_loss: 1.69935e+01
2025-07-07 17:59:56,929 - step: 59, training_loss: 1.81961e+01
2025-07-07 17:59:58,991 - step: 60, training_loss: 1.86910e+01
2025-07-07 18:00:01,065 - step: 61, training_loss: 1.64431e+01
2025-07-07 18:00:03,149 - step: 62, training_loss: 2.05848e+01
2025-07-07 18:00:05,214 - step: 63, training_loss: 1.70963e+01
2025-07-07 18:00:07,292 - step: 64, training_loss: 2.04367e+01
2025-07-07 18:00:09,371 - step: 65, training_loss: 1.78296e+01
2025-07-07 18:00:11,471 - step: 66, training_loss: 1.69648e+01
2025-07-07 18:00:13,561 - step: 67, training_loss: 1.63620e+01
2025-07-07 18:00:15,649 - step: 68, training_loss: 1.88219e+01
2025-07-07 18:00:17,757 - step: 69, training_loss: 1.81038e+01
2025-07-07 18:00:19,872 - step: 70, training_loss: 1.82797e+01
2025-07-07 18:00:21,976 - step: 71, training_loss: 1.91926e+01
2025-07-07 18:00:24,090 - step: 72, training_loss: 2.02060e+01
2025-07-07 18:00:26,202 - step: 73, training_loss: 1.64836e+01
2025-07-07 18:00:28,323 - step: 74, training_loss: 1.79916e+01
2025-07-07 18:00:30,424 - step: 75, training_loss: 1.86977e+01
2025-07-07 18:00:32,578 - step: 76, training_loss: 1.87156e+01
2025-07-07 18:00:34,690 - step: 77, training_loss: 1.79115e+01
2025-07-07 18:00:36,818 - step: 78, training_loss: 1.82406e+01
2025-07-07 18:00:38,971 - step: 79, training_loss: 1.84407e+01
2025-07-07 18:00:41,095 - step: 80, training_loss: 1.99719e+01
2025-07-07 18:00:43,191 - step: 81, training_loss: 1.75615e+01
2025-07-07 18:00:45,339 - step: 82, training_loss: 1.80740e+01
2025-07-07 18:00:47,480 - step: 83, training_loss: 1.94333e+01
2025-07-07 18:00:49,644 - step: 84, training_loss: 1.91228e+01
2025-07-07 18:00:51,817 - step: 85, training_loss: 1.99572e+01
2025-07-07 18:00:53,991 - step: 86, training_loss: 1.84631e+01
2025-07-07 18:00:56,141 - step: 87, training_loss: 1.57030e+01
2025-07-07 18:00:58,291 - step: 88, training_loss: 1.84497e+01
2025-07-07 18:01:00,448 - step: 89, training_loss: 1.78099e+01
2025-07-07 18:01:02,615 - step: 90, training_loss: 1.83837e+01
2025-07-07 18:01:04,780 - step: 91, training_loss: 1.78889e+01
2025-07-07 18:01:06,928 - step: 92, training_loss: 1.92065e+01
2025-07-07 18:01:09,100 - step: 93, training_loss: 1.87088e+01
2025-07-07 18:01:11,266 - step: 94, training_loss: 1.85064e+01
2025-07-07 18:01:13,435 - step: 95, training_loss: 2.05196e+01
2025-07-07 18:01:15,586 - step: 96, training_loss: 1.85247e+01
2025-07-07 18:01:17,760 - step: 97, training_loss: 1.69550e+01
2025-07-07 18:01:19,948 - step: 98, training_loss: 1.87407e+01
2025-07-07 18:01:22,149 - step: 99, training_loss: 1.76494e+01
2025-07-07 18:01:24,332 - step: 100, training_loss: 1.69133e+01
2025-07-07 18:01:24,757 - step: 100, evaluation_loss: 1.80509e+01
2025-07-07 18:01:26,959 - step: 101, training_loss: 1.78765e+01
2025-07-07 18:01:29,161 - step: 102, training_loss: 1.77588e+01
2025-07-07 18:01:31,367 - step: 103, training_loss: 1.93158e+01
2025-07-07 18:01:33,580 - step: 104, training_loss: 1.97793e+01
2025-07-07 18:01:35,778 - step: 105, training_loss: 1.79655e+01
2025-07-07 18:01:37,989 - step: 106, training_loss: 1.92561e+01
2025-07-07 18:01:40,199 - step: 107, training_loss: 1.86665e+01
2025-07-07 18:01:42,456 - step: 108, training_loss: 1.80970e+01
2025-07-07 18:01:44,759 - step: 109, training_loss: 1.96175e+01
2025-07-07 18:01:47,040 - step: 110, training_loss: 1.99687e+01
2025-07-07 18:01:49,352 - step: 111, training_loss: 1.75409e+01
2025-07-07 18:01:51,593 - step: 112, training_loss: 1.93848e+01
2025-07-07 18:01:53,849 - step: 113, training_loss: 1.96227e+01
2025-07-07 18:01:56,075 - step: 114, training_loss: 1.84121e+01
